{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the course webpage Welcome to the course COV 816 - Image Processing webpage Course Materials Github Repository Reference Books Course's Software Course's Slides Course's Videos","title":"Home"},{"location":"#welcome-to-the-course-webpage","text":"Welcome to the course COV 816 - Image Processing webpage","title":"Welcome to the course webpage"},{"location":"#course-materials","text":"Github Repository Reference Books Course's Software Course's Slides Course's Videos","title":"Course Materials"},{"location":"COV%20816/01_B01/","text":"1 BASICS OF IMAGES 1.1 Definition of a Digital Image An image is a two-dimensional array of values representing light intensity. For the purposes of image processing, the term image refers to a digital image. An image is a function of the light intensity: f(x, y) where f is the brightness of the point \\((x, y\\)), and x and y represent the spatial coordinates of a picture element (abbreviated pixel). By convention, the spatial reference of the pixel with the coordinates (0, 0) is located at the top, left corner of the image. Notice in Figure 1-1 that the value of x increases moving from left to right, and the value of y increases from top to bottom. Figure 1-1 . Spatial Reference of the (0, 0) Pixel. In digital image processing, an imaging sensor converts an image into a discrete number of pixels. The imaging sensor assigns to each pixel a numeric location and a gray level or color value that specifies the brightness or color of the pixel. 1.2 Properties of a Digitized Image A digitized image has three basic properties: resolution , definition , and number of planes . 1.2.1 Image Resolution The spatial resolution of an image is its number of rows and columns of pixels. An image composed of m columns and n rows has a resolution of \\(m\\times n\\). This image has m pixels along its horizontal axis and n pixels along its vertical axis. 1.2.2 Image Definition The definition of an image indicates the number of shades that you can see in the image. The bit depth of an image is the number of bits used to encode the value of a pixel. For a given bit depth of \\(n\\), the image has an image definition of \\(2^n\\), meaning a pixel can have \\(2^n\\) different values. For example, if \\(n\\) equals 8-bits, a pixel can take 256 different values ranging from 0 to 255. If \\(n\\) equals 16 bits, a pixel can take 65,536 different values ranging from 0 to 65,535 or from \u201332,768 to 32,767. The manner in which you encode your image depends on the nature of the image acquisition device, the type of image processing you need to use, and the type of analysis you need to perform. For example, 8-bit encoding is sufficient if you need to obtain the shape information of objects in an image. However, if you need to precisely measure the light intensity of an image or region in an image, you must use 16-bit or floating-point encoding. TIP: Use color encoded images when your machine vision or image processing application depends on the color content of the objects you are inspecting or analyzing. 1.2.3 Number of Planes The number of planes in an image corresponds to the number of arrays of pixels that compose the image. A grayscale or pseudo-color image is composed of one plane, while a true-color image is composed of three planes\u2014one each for the red component, blue component, and green component. In true-color images, the color component intensities of a pixel are coded into three different values. The color image is the combination of three arrays of pixels corresponding to the red , green , and blue components in an RGB image. HSL images are defined by their hue , saturation , and luminance values. 1.3 Image Types We can talk about three main types of images: grayscale, color, and complex images . 1.3.1 Grayscale Images A grayscale image is composed of a single plane of pixels. Each pixel is encoded using one of the following single numbers: An 8-bit unsigned integer representing grayscale values between 0 and 255. A 16-bit signed integer representing grayscale values between \u201332,768 and +32,767. A single precision floating point number (encoded using four bytes) representing grayscale values ranging from \\(-\\infty\\) to \\(+\\infty\\). 1.3.2 Color Images A color image is encoded in memory as either an RGB or HSL image. Color image pixels are a composite of four values. RGB images store color information using 8 bits each for the red , green , and blue planes. HSL images store color information using 8 bits each for hue , saturation , and luminance . In all of the color models, an additional 8-bit value goes unused. This representation is known as 4\\(\\times\\)8-bit or 32-bit encoding. 1.3.3 Complex Images A complex image contains the frequency information of a grayscale image. Create a complex image by applying a Fast Fourier transform (FFT) to a grayscale image. When you transform a grayscale image into a complex image, you can perform frequency domain operations on the image. Each pixel in a complex image is encoded as two single-precision floating-point values, which represent the real and imaginary components of the complex pixel. You can extract the following four components from a complex image: the real part , imaginary part , magnitude , and phase . 1.4 Image Files An image file is composed of a header followed by pixel values. Depending on the file format, the header contains image information about the horizontal and vertical resolution, pixel definition, and the original palette. Image files may also store information about calibration, pattern matching templates, and overlays. The following are common image file formats: Bitmap ( BMP ) Tagged image file format ( TIFF ). Portable network graphics ( PNG ): Offers the capability of storing image information about spatial calibration, pattern matching templates, and overlays. Joint Photographic Experts Group format ( JPEG ). National Instruments image file format ( AIPD ): Used for saving floating-point, complex, and HSL images. Standard formats for 8-bit grayscale and RGB color images are BMP, TIFF, PNG, JPEG, and AIPD. Standard formats for 16-bit grayscale and complex images are PNG and AIPD. 1.5 Color Spaces Color spaces allow you to represent a color. A color space is a subspace within a three-dimensional coordinate system where each color is represented by a point. You can use color spaces to facilitate the description of colors between persons, machines, or software programs. Various industries and applications use a number of different color spaces. Humans perceive color according to parameters such as brightness, hue, and intensity, while computers perceive color as a combination of red, green, and blue. The printing industry uses cyan, magenta, and yellow to specify color. The following is a list of common color spaces. RGB: Based on red, green, and blue. Used by computers to display images. HSL: Based on hue, saturation, and luminance. Used in image processing applications. CIE: Based on brightness, hue, and colorfulness. Defined by the Commission Internationale de l\u2019Eclairage (International Commission on Illumination) as the different sensations of color that the human brain perceives. CMY: Based on cyan, magenta, and yellow. Used by the printing industry. YIQ: Separates the luminance information (Y) from the color information (I and Q). Used for TV broadcasting. When to Use: You must define a color space every time you process color images. If you expect the lighting conditions to vary considerably during your color machine vision application, use the HSL color space. The HSL color space provides more accurate color information than the RGB space when running color processing functions, such as color matching, color location, and color pattern matching. If you do not expect the lighting conditions to vary considerably during your application, and you can easily define the colors you are looking for using red, green, and blue, use the RGB space. Also, use the RGB space if you want only to display color images, but not process them, in your application. The RGB space reproduces an image as you would expect to see it. 1.5.1 Concepts Because color is the brain\u2019s reaction to a specific visual stimulus, color is best described by the different sensations of color that the human brain perceives. The color-sensitive cells in the eye\u2019s retina sample color using three bands that correspond to red, green, and blue light. The signals from these cells travel to the brain where they combine to produce different sensations of colors. The Commission Internationale de l\u2019Eclairage has defined the following sensations: Brightness: The sensation of an area exhibiting more or less light Hue: The sensation of an area appearing similar to a combination of red, green, and blue Colorfulness: The sensation of an area appearing to exhibit more or less of its hue Lightness: The sensation of an area\u2019s brightness relative to a reference white in the scene Chroma: The colorfulness of an area with respect to a reference white in the scene Saturation: The colorfulness of an area relative to its brightness The trichromatic theory describes how three separate lights (red, green, and blue) can be combined to match any visible color. This theory is based on the three color sensors that the eye uses. Printing and photography use the trichromatic theory as the basis for combining three different colored dyes to reproduce colors in a scene. Computer color spaces also use three parameters to define a color. Most color spaces are geared toward displaying images with hardware, such as color monitors and printers, or toward applications that manipulate color information, such as computer graphics and image processing. Color CRT monitors, major of color video cameras, and most computer graphics systems use the RGB color space. The HSL space, combined with RGB and YIQ , is frequently used in applications that manipulate color, such as image processing. The color picture publishing industry uses the CMY color space, also known as CMYK . The YIQ space is the standard for color TV broadcast. 1.5.2 The RGB Color Space The RGB color space is the most commonly used color space. The human eye receives color information in separate red , green , and blue components through cones, the color receptors present in the human eye. These three colors are known as additive primary colors. In an additive color system, the human brain processes the three primary light sources and combines them to compose a single color image . The three primary color components can combine to reproduce almost all other possible colors. You can visualize the RGB space as a 3-dimensional cube with red, green, and blue at the corners of each axis, as shown in Figure 1.5-1. Black is at the origin, while white is at the opposite corner of the cube. Each side of the cube has a value between 0 and 1. Along each axis of the RGB cube, the colors range from no contribution of that component to a fully saturated color. Any point, or color, within the cube is specified by three numbers: an R , G , B triple. The diagonal line of the cube from black \\((0, 0, 0)\\) to white \\((1, 1, 1\\)) represents all the grayscale values or where all of the red, green, and blue components are equal. Different computer hardware and software combinations use different ranges for the colors. Common combinations are 0\u2013255 and 0\u201365,535 for each component. To map color values within these ranges to values in the RGB cube, divide the color values by the maximum value that the range can take. Figure 1.5-1. RGB Cube. The RGB color space lies within the perceptual space of humans. In other words, the RGB cube represents fewer colors than we can see. The RGB space simplifies the design of computer monitors, but it is not ideal for all applications. In the RGB color space, the red, green, and blue color components are all necessary to describe a color. Therefore, RGB is not as intuitive as other color spaces. The HSL color space describes color using only the hue component, which makes HSL the best choice for many image processing applications, such as color matching. 1.5.3 HSL Color Space The HSL color space was developed to put color in terms that are easy for humans to quantify . H ue, S aturation, and L ightness are characteristics that distinguish one color from another in the HSL space. Hue corresponds to the dominant wavelength of the color. The hue component is a color, such as orange, green, or violet. You can visualize the range of hues as a rainbow. Saturation refers to the amount of white added to the hue and represents the relative purity of a color. A color without any white is fully saturated. The degree of saturation is inversely proportional to the amount of white light added. Colors such as pink (red and white) and lavender (purple and white) are less saturated than red and purple. Lightness embodies the chromatic notion of luminance, or the amplitude or power of light. Chromaticity is the combination of hue and saturation, and the relationship between chromacity and lightness characterizes a color. Systems that manipulate hue use the HSL color space, which also can be written as HSB (Hue, Saturation, and Brightness) or HSV (Hue, Saturation, and Value). 1.5.4 CIE L*a*b* Color Space CIE 1976 L*a*b* , one of the CIE-based color spaces, is a way to linearize the perceptibility of color differences. The nonlinear relations for L* , a* , and b* mimic the logarithmic response of the eye. 1.5.5 CMY Color Space CMY is another set of familiar primary colors: cyan, magenta, and yellow. CMY is a subtractive color space in which these primary colors are subtracted from white light to produce the desired color. The CMY color space is the basis of most color printing and photography processes. CMY is the complement of the RGB color space because cyan, magenta, and yellow are the complements of red, green, and blue . 1.5.6 YIQ Color Space The YIQ space is the primary color space adopted by the National Television System Committee (NTSC) for color TV broadcasting. It is a linear transformation of the RGB cube for transmission efficiency and for maintaining compatibility with monochrome television standards. The Y component of the YIQ system provides all the video information that a monochrome television set requires. The main advantage of the YIQ space for image processing is that the luminance information (Y) is decoupled from the color information (I and Q). Because luminance is proportional to the amount of light perceived by the eye, modifications to the grayscale appearance of the image do not affect the color information.","title":"Unit 01 - Basic Concepts"},{"location":"COV%20816/01_B01/#1-basics-of-images","text":"","title":"1 BASICS OF IMAGES"},{"location":"COV%20816/01_B01/#11-definition-of-a-digital-image","text":"An image is a two-dimensional array of values representing light intensity. For the purposes of image processing, the term image refers to a digital image. An image is a function of the light intensity: f(x, y) where f is the brightness of the point \\((x, y\\)), and x and y represent the spatial coordinates of a picture element (abbreviated pixel). By convention, the spatial reference of the pixel with the coordinates (0, 0) is located at the top, left corner of the image. Notice in Figure 1-1 that the value of x increases moving from left to right, and the value of y increases from top to bottom. Figure 1-1 . Spatial Reference of the (0, 0) Pixel. In digital image processing, an imaging sensor converts an image into a discrete number of pixels. The imaging sensor assigns to each pixel a numeric location and a gray level or color value that specifies the brightness or color of the pixel.","title":"1.1 Definition of a Digital Image"},{"location":"COV%20816/01_B01/#12-properties-of-a-digitized-image","text":"A digitized image has three basic properties: resolution , definition , and number of planes .","title":"1.2 Properties of a Digitized Image"},{"location":"COV%20816/01_B01/#121-image-resolution","text":"The spatial resolution of an image is its number of rows and columns of pixels. An image composed of m columns and n rows has a resolution of \\(m\\times n\\). This image has m pixels along its horizontal axis and n pixels along its vertical axis.","title":"1.2.1 Image Resolution"},{"location":"COV%20816/01_B01/#122-image-definition","text":"The definition of an image indicates the number of shades that you can see in the image. The bit depth of an image is the number of bits used to encode the value of a pixel. For a given bit depth of \\(n\\), the image has an image definition of \\(2^n\\), meaning a pixel can have \\(2^n\\) different values. For example, if \\(n\\) equals 8-bits, a pixel can take 256 different values ranging from 0 to 255. If \\(n\\) equals 16 bits, a pixel can take 65,536 different values ranging from 0 to 65,535 or from \u201332,768 to 32,767. The manner in which you encode your image depends on the nature of the image acquisition device, the type of image processing you need to use, and the type of analysis you need to perform. For example, 8-bit encoding is sufficient if you need to obtain the shape information of objects in an image. However, if you need to precisely measure the light intensity of an image or region in an image, you must use 16-bit or floating-point encoding. TIP: Use color encoded images when your machine vision or image processing application depends on the color content of the objects you are inspecting or analyzing.","title":"1.2.2 Image Definition"},{"location":"COV%20816/01_B01/#123-number-of-planes","text":"The number of planes in an image corresponds to the number of arrays of pixels that compose the image. A grayscale or pseudo-color image is composed of one plane, while a true-color image is composed of three planes\u2014one each for the red component, blue component, and green component. In true-color images, the color component intensities of a pixel are coded into three different values. The color image is the combination of three arrays of pixels corresponding to the red , green , and blue components in an RGB image. HSL images are defined by their hue , saturation , and luminance values.","title":"1.2.3 Number of Planes"},{"location":"COV%20816/01_B01/#13-image-types","text":"We can talk about three main types of images: grayscale, color, and complex images .","title":"1.3 Image Types"},{"location":"COV%20816/01_B01/#131-grayscale-images","text":"A grayscale image is composed of a single plane of pixels. Each pixel is encoded using one of the following single numbers: An 8-bit unsigned integer representing grayscale values between 0 and 255. A 16-bit signed integer representing grayscale values between \u201332,768 and +32,767. A single precision floating point number (encoded using four bytes) representing grayscale values ranging from \\(-\\infty\\) to \\(+\\infty\\).","title":"1.3.1 Grayscale Images"},{"location":"COV%20816/01_B01/#132-color-images","text":"A color image is encoded in memory as either an RGB or HSL image. Color image pixels are a composite of four values. RGB images store color information using 8 bits each for the red , green , and blue planes. HSL images store color information using 8 bits each for hue , saturation , and luminance . In all of the color models, an additional 8-bit value goes unused. This representation is known as 4\\(\\times\\)8-bit or 32-bit encoding.","title":"1.3.2 Color Images"},{"location":"COV%20816/01_B01/#133-complex-images","text":"A complex image contains the frequency information of a grayscale image. Create a complex image by applying a Fast Fourier transform (FFT) to a grayscale image. When you transform a grayscale image into a complex image, you can perform frequency domain operations on the image. Each pixel in a complex image is encoded as two single-precision floating-point values, which represent the real and imaginary components of the complex pixel. You can extract the following four components from a complex image: the real part , imaginary part , magnitude , and phase .","title":"1.3.3 Complex Images"},{"location":"COV%20816/01_B01/#14-image-files","text":"An image file is composed of a header followed by pixel values. Depending on the file format, the header contains image information about the horizontal and vertical resolution, pixel definition, and the original palette. Image files may also store information about calibration, pattern matching templates, and overlays. The following are common image file formats: Bitmap ( BMP ) Tagged image file format ( TIFF ). Portable network graphics ( PNG ): Offers the capability of storing image information about spatial calibration, pattern matching templates, and overlays. Joint Photographic Experts Group format ( JPEG ). National Instruments image file format ( AIPD ): Used for saving floating-point, complex, and HSL images. Standard formats for 8-bit grayscale and RGB color images are BMP, TIFF, PNG, JPEG, and AIPD. Standard formats for 16-bit grayscale and complex images are PNG and AIPD.","title":"1.4 Image Files"},{"location":"COV%20816/01_B01/#15-color-spaces","text":"Color spaces allow you to represent a color. A color space is a subspace within a three-dimensional coordinate system where each color is represented by a point. You can use color spaces to facilitate the description of colors between persons, machines, or software programs. Various industries and applications use a number of different color spaces. Humans perceive color according to parameters such as brightness, hue, and intensity, while computers perceive color as a combination of red, green, and blue. The printing industry uses cyan, magenta, and yellow to specify color. The following is a list of common color spaces. RGB: Based on red, green, and blue. Used by computers to display images. HSL: Based on hue, saturation, and luminance. Used in image processing applications. CIE: Based on brightness, hue, and colorfulness. Defined by the Commission Internationale de l\u2019Eclairage (International Commission on Illumination) as the different sensations of color that the human brain perceives. CMY: Based on cyan, magenta, and yellow. Used by the printing industry. YIQ: Separates the luminance information (Y) from the color information (I and Q). Used for TV broadcasting. When to Use: You must define a color space every time you process color images. If you expect the lighting conditions to vary considerably during your color machine vision application, use the HSL color space. The HSL color space provides more accurate color information than the RGB space when running color processing functions, such as color matching, color location, and color pattern matching. If you do not expect the lighting conditions to vary considerably during your application, and you can easily define the colors you are looking for using red, green, and blue, use the RGB space. Also, use the RGB space if you want only to display color images, but not process them, in your application. The RGB space reproduces an image as you would expect to see it.","title":"1.5 Color Spaces"},{"location":"COV%20816/01_B01/#151-concepts","text":"Because color is the brain\u2019s reaction to a specific visual stimulus, color is best described by the different sensations of color that the human brain perceives. The color-sensitive cells in the eye\u2019s retina sample color using three bands that correspond to red, green, and blue light. The signals from these cells travel to the brain where they combine to produce different sensations of colors. The Commission Internationale de l\u2019Eclairage has defined the following sensations: Brightness: The sensation of an area exhibiting more or less light Hue: The sensation of an area appearing similar to a combination of red, green, and blue Colorfulness: The sensation of an area appearing to exhibit more or less of its hue Lightness: The sensation of an area\u2019s brightness relative to a reference white in the scene Chroma: The colorfulness of an area with respect to a reference white in the scene Saturation: The colorfulness of an area relative to its brightness The trichromatic theory describes how three separate lights (red, green, and blue) can be combined to match any visible color. This theory is based on the three color sensors that the eye uses. Printing and photography use the trichromatic theory as the basis for combining three different colored dyes to reproduce colors in a scene. Computer color spaces also use three parameters to define a color. Most color spaces are geared toward displaying images with hardware, such as color monitors and printers, or toward applications that manipulate color information, such as computer graphics and image processing. Color CRT monitors, major of color video cameras, and most computer graphics systems use the RGB color space. The HSL space, combined with RGB and YIQ , is frequently used in applications that manipulate color, such as image processing. The color picture publishing industry uses the CMY color space, also known as CMYK . The YIQ space is the standard for color TV broadcast.","title":"1.5.1 Concepts"},{"location":"COV%20816/01_B01/#152-the-rgb-color-space","text":"The RGB color space is the most commonly used color space. The human eye receives color information in separate red , green , and blue components through cones, the color receptors present in the human eye. These three colors are known as additive primary colors. In an additive color system, the human brain processes the three primary light sources and combines them to compose a single color image . The three primary color components can combine to reproduce almost all other possible colors. You can visualize the RGB space as a 3-dimensional cube with red, green, and blue at the corners of each axis, as shown in Figure 1.5-1. Black is at the origin, while white is at the opposite corner of the cube. Each side of the cube has a value between 0 and 1. Along each axis of the RGB cube, the colors range from no contribution of that component to a fully saturated color. Any point, or color, within the cube is specified by three numbers: an R , G , B triple. The diagonal line of the cube from black \\((0, 0, 0)\\) to white \\((1, 1, 1\\)) represents all the grayscale values or where all of the red, green, and blue components are equal. Different computer hardware and software combinations use different ranges for the colors. Common combinations are 0\u2013255 and 0\u201365,535 for each component. To map color values within these ranges to values in the RGB cube, divide the color values by the maximum value that the range can take. Figure 1.5-1. RGB Cube. The RGB color space lies within the perceptual space of humans. In other words, the RGB cube represents fewer colors than we can see. The RGB space simplifies the design of computer monitors, but it is not ideal for all applications. In the RGB color space, the red, green, and blue color components are all necessary to describe a color. Therefore, RGB is not as intuitive as other color spaces. The HSL color space describes color using only the hue component, which makes HSL the best choice for many image processing applications, such as color matching.","title":"1.5.2 The RGB Color Space"},{"location":"COV%20816/01_B01/#153-hsl-color-space","text":"The HSL color space was developed to put color in terms that are easy for humans to quantify . H ue, S aturation, and L ightness are characteristics that distinguish one color from another in the HSL space. Hue corresponds to the dominant wavelength of the color. The hue component is a color, such as orange, green, or violet. You can visualize the range of hues as a rainbow. Saturation refers to the amount of white added to the hue and represents the relative purity of a color. A color without any white is fully saturated. The degree of saturation is inversely proportional to the amount of white light added. Colors such as pink (red and white) and lavender (purple and white) are less saturated than red and purple. Lightness embodies the chromatic notion of luminance, or the amplitude or power of light. Chromaticity is the combination of hue and saturation, and the relationship between chromacity and lightness characterizes a color. Systems that manipulate hue use the HSL color space, which also can be written as HSB (Hue, Saturation, and Brightness) or HSV (Hue, Saturation, and Value).","title":"1.5.3 HSL Color Space"},{"location":"COV%20816/01_B01/#154-cie-lab-color-space","text":"CIE 1976 L*a*b* , one of the CIE-based color spaces, is a way to linearize the perceptibility of color differences. The nonlinear relations for L* , a* , and b* mimic the logarithmic response of the eye.","title":"1.5.4 CIE L*a*b* Color Space"},{"location":"COV%20816/01_B01/#155-cmy-color-space","text":"CMY is another set of familiar primary colors: cyan, magenta, and yellow. CMY is a subtractive color space in which these primary colors are subtracted from white light to produce the desired color. The CMY color space is the basis of most color printing and photography processes. CMY is the complement of the RGB color space because cyan, magenta, and yellow are the complements of red, green, and blue .","title":"1.5.5 CMY Color Space"},{"location":"COV%20816/01_B01/#156-yiq-color-space","text":"The YIQ space is the primary color space adopted by the National Television System Committee (NTSC) for color TV broadcasting. It is a linear transformation of the RGB cube for transmission efficiency and for maintaining compatibility with monochrome television standards. The Y component of the YIQ system provides all the video information that a monochrome television set requires. The main advantage of the YIQ space for image processing is that the luminance information (Y) is decoupled from the color information (I and Q). Because luminance is proportional to the amount of light perceived by the eye, modifications to the grayscale appearance of the image do not affect the color information.","title":"1.5.6 YIQ Color Space"},{"location":"COV%20816/02_B02/","text":"2 IMAGE ACQUISITION This block section addressess how to set up an imaging system and calibrate the imaging setup so that you can convert pixel-coordinates to real-world coordinates. Converting pixel-coordinates to real-world coordinates is useful when you need to make accurate measurements from inspection images using real-world units 2.1 System Setup Before acquire, analyze, and process images, is necessary to set up an imaging system. Five factors comprise a imaging system: field of view, working distance, resolution, depth of field, and sensor size. Following figure illustrates these concepts: Figure 2.1-1. Fundamental Parameters of an Imaging System. Resolution : The smallest feature size on your object that the imaging system can distinguish. Pixel resolution : The minimum number of pixels needed to represent the object under inspection. Field of view : The area of the object under inspection that the camera can acquire. Working distance : The distance from the front of the camera lens to the object under inspection. Sensor size : The size of a sensor\u2019s active area, typically defined by the sensor's horizontal dimension. Depth of field : The maximum object depth that remains in focus. 2.2 Acquiring Quality Images The manner in which you set up your system depends on the type of analysis and processing you need to do. Your imaging system should produce images with high enough quality so that you can extract the information you need from the images. Five factors contribute to overall image quality: resolution, contrast, depth of field, perspective, and distortion. 2.2.1 Resolution There are two kinds of resolution to consider when setting up your imaging system: pixel resolution and resolution. Pixel resolution refers to the minimum number of pixels you need to represent the object under inspection. You can determine the pixel resolution you need by the smallest feature you need to inspect. Try to have at least two pixels represent the smallest feature. You can use the following equation to determine the minimum pixel resolution required by your imaging system: \\frac{\\text{length of object's longest axis}}{\\text{size of object's smallest feature}}\\times 2 If the object does not occupy the entire field of view, the image size will be greater than the pixel resolution. Resolution indicates the amount of object detail that the imaging system can reproduce. Images with low resolution lack detail and often appear blurry. Three factors contribute to the resolution of your imaging system: field of view, the camera sensor size, and number of pixels in the sensor. When you know these three factors, you can determine the focal length of your camera lens. 2.2.2 Field of View The field of view is the area of the object under inspection that the camera can acquire. Figure 2.2-1 describes the relationship between pixel resolution and the field of view. Figure 2.2-1. Relationship between Pixel Resolution and Field of View. Figure 2.2-1a shows an object that occupies the field of view. Figure 2.2-1b shows an object that occupies less space than the field of view. If \\(w\\) is the size of the smallest feature in the \\(x\\) direction and \\(h\\) is the size of the smallest feature in the \\(y\\) direction, the minimum \\(x\\) pixel resolution is: \\frac{w_{fov}}{w}\\times 2 and the minimum \\(y\\) pixel resolution is: \\frac{h_{fov}}{h}\\times 2 Choose the larger pixel resolution of the two for your imaging application. 2.2.3 Sensor Size and Number of Pixels in the Sensor The camera sensor size is important in determining your field of view, which is a key element in determining your minimum resolution requirement. The sensor\u2019s diagonal length specifies the size of the sensor\u2019s active area. The number of pixels in your sensor should be greater than or equal to the pixel resolution. Choose a camera with a sensor that satisfies your minimum resolution requirement. 2.2.4 Lens Focal Length When you determine the field of view and appropriate sensor size, you can decide which type of camera lens meets your imaging needs. A lens is defined primarily by its focal length. The relationship between the lens, field of view, and sensor size is as follows: f=\\frac{\\text{sensor size} \\times \\text{working distance}}{\\text{field of view}} If you cannot change the working distance, you are limited in choosing a focal length for your lens. If you have a fixed working distance and your focal length is short, your images may appear distorted. However, if you have the flexibility to change your working distance, modify the distance so that you can select a lens with the appropriate focal length and minimize distortion. 2.2.5 Contrast Resolution and contrast are closely related factors contributing to image quality. Contrast defines the differences in intensity values between the object under inspection and the background. Your imaging system should have enough contrast to distinguish objects from the background. Proper lighting techniques can enhance the contrast of your system. 2.2.6 Depth of Field The depth of field of a lens is its ability to keep objects of varying heights in focus. If you need to inspect objects with various heights, chose a lens that can maintain the image quality you need as the objects move closer to and further from the lens. 2.2.7 Perspective Perspective errors often occur when the camera axis is not perpendicular to the object you are inspecting. Figure 3-3a shows an ideal camera position. Figure 3-3b shows a camera imaging an object from an angle. Figure 2.2-2. Camera Angle Relative to the Object of Interest. 2.3 Calibration What is camera calibration? The process of estimating the parameters of a camera is called camera calibration. This means we have all the information (parameters or coefficients) about the camera required to determine an accurate relationship between a 3D point in the real world and its corresponding 2D projection (pixel) in the image captured by that calibrated camera. Typically this means recovering two kinds of parameters Internal parameters of the camera/lens system. E.g. focal length, optical center, and radial distortion coefficients of the lens. External parameters : This refers to the orientation (rotation and translation) of the camera with respect to some world coordinate system. 2.3.1 Basics Some pinhole cameras introduce significant distortion to images. Two major kinds of distortion are radial distortion and tangential distortion. Radial distortion causes straight lines to appear curved. Radial distortion becomes larger the farther points are from the center of the image. For example, one image is shown below in which two edges of a chess board are marked with red lines. But, you can see that the border of the chess board is not a straight line and doesn't match with the red line. All the expected straight lines are bulged out. Figure 2.3-1. Radial Distortion. Radial distortion can be represented as follows: x_{distorted} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\\\ y_{distorted} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) Similarly, tangential distortion occurs because the image-taking lense is not aligned perfectly parallel to the imaging plane. So, some areas in the image may look nearer than expected. The amount of tangential distortion can be represented as below: x_{distorted} = x + [ 2p_1xy + p_2(r^2+2x^2)] \\\\ y_{distorted} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy] In short, we need to find five parameters, known as distortion coefficients given by: \\text{Distortion coefficients}=(k_1 \\hspace{10pt} k_2 \\hspace{10pt} p_1 \\hspace{10pt} p_2 \\hspace{10pt} k_3) In addition to this, we need to some other information, like the intrinsic and extrinsic parameters of the camera. Intrinsic parameters are specific to a camera. They include information like focal length (\\(f_x\\), \\(f_y\\)) and optical centers (\\(c_x\\), \\(c_y\\)). The focal length and optical centers can be used to create a camera matrix, which can be used to remove distortion due to the lenses of a specific camera. The camera matrix is unique to a specific camera, so once calculated, it can be reused on other images taken by the same camera. It is expressed as a \\(3\\times 3\\) matrix: \\text{Camera matrix} = \\left [ \\begin{matrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{matrix} \\right ] Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system. 2.4 References [1] BRADSKI, G. The OpenCV Library , Dr. Dobb\u2019s Journal of Software Tools, 2000. [2] BURGER, W., BURGE, M. J. Principles of digital image processing. 1: Fundamental techniques . London, Springer, 2009. [3] NATIONAL INSTRUMENTS. IMAQ: Vision Concepts Manual . Austin, Texas, 2003.","title":"Unit 02 - Image Aquisition"},{"location":"COV%20816/02_B02/#2-image-acquisition","text":"This block section addressess how to set up an imaging system and calibrate the imaging setup so that you can convert pixel-coordinates to real-world coordinates. Converting pixel-coordinates to real-world coordinates is useful when you need to make accurate measurements from inspection images using real-world units","title":"2 IMAGE ACQUISITION"},{"location":"COV%20816/02_B02/#21-system-setup","text":"Before acquire, analyze, and process images, is necessary to set up an imaging system. Five factors comprise a imaging system: field of view, working distance, resolution, depth of field, and sensor size. Following figure illustrates these concepts: Figure 2.1-1. Fundamental Parameters of an Imaging System. Resolution : The smallest feature size on your object that the imaging system can distinguish. Pixel resolution : The minimum number of pixels needed to represent the object under inspection. Field of view : The area of the object under inspection that the camera can acquire. Working distance : The distance from the front of the camera lens to the object under inspection. Sensor size : The size of a sensor\u2019s active area, typically defined by the sensor's horizontal dimension. Depth of field : The maximum object depth that remains in focus.","title":"2.1 System Setup"},{"location":"COV%20816/02_B02/#22-acquiring-quality-images","text":"The manner in which you set up your system depends on the type of analysis and processing you need to do. Your imaging system should produce images with high enough quality so that you can extract the information you need from the images. Five factors contribute to overall image quality: resolution, contrast, depth of field, perspective, and distortion.","title":"2.2 Acquiring Quality Images"},{"location":"COV%20816/02_B02/#221-resolution","text":"There are two kinds of resolution to consider when setting up your imaging system: pixel resolution and resolution. Pixel resolution refers to the minimum number of pixels you need to represent the object under inspection. You can determine the pixel resolution you need by the smallest feature you need to inspect. Try to have at least two pixels represent the smallest feature. You can use the following equation to determine the minimum pixel resolution required by your imaging system: \\frac{\\text{length of object's longest axis}}{\\text{size of object's smallest feature}}\\times 2 If the object does not occupy the entire field of view, the image size will be greater than the pixel resolution. Resolution indicates the amount of object detail that the imaging system can reproduce. Images with low resolution lack detail and often appear blurry. Three factors contribute to the resolution of your imaging system: field of view, the camera sensor size, and number of pixels in the sensor. When you know these three factors, you can determine the focal length of your camera lens.","title":"2.2.1 Resolution"},{"location":"COV%20816/02_B02/#222-field-of-view","text":"The field of view is the area of the object under inspection that the camera can acquire. Figure 2.2-1 describes the relationship between pixel resolution and the field of view. Figure 2.2-1. Relationship between Pixel Resolution and Field of View. Figure 2.2-1a shows an object that occupies the field of view. Figure 2.2-1b shows an object that occupies less space than the field of view. If \\(w\\) is the size of the smallest feature in the \\(x\\) direction and \\(h\\) is the size of the smallest feature in the \\(y\\) direction, the minimum \\(x\\) pixel resolution is: \\frac{w_{fov}}{w}\\times 2 and the minimum \\(y\\) pixel resolution is: \\frac{h_{fov}}{h}\\times 2 Choose the larger pixel resolution of the two for your imaging application.","title":"2.2.2 Field of View"},{"location":"COV%20816/02_B02/#223-sensor-size-and-number-of-pixels-in-the-sensor","text":"The camera sensor size is important in determining your field of view, which is a key element in determining your minimum resolution requirement. The sensor\u2019s diagonal length specifies the size of the sensor\u2019s active area. The number of pixels in your sensor should be greater than or equal to the pixel resolution. Choose a camera with a sensor that satisfies your minimum resolution requirement.","title":"2.2.3 Sensor Size and Number of Pixels in the Sensor"},{"location":"COV%20816/02_B02/#224-lens-focal-length","text":"When you determine the field of view and appropriate sensor size, you can decide which type of camera lens meets your imaging needs. A lens is defined primarily by its focal length. The relationship between the lens, field of view, and sensor size is as follows: f=\\frac{\\text{sensor size} \\times \\text{working distance}}{\\text{field of view}} If you cannot change the working distance, you are limited in choosing a focal length for your lens. If you have a fixed working distance and your focal length is short, your images may appear distorted. However, if you have the flexibility to change your working distance, modify the distance so that you can select a lens with the appropriate focal length and minimize distortion.","title":"2.2.4 Lens Focal Length"},{"location":"COV%20816/02_B02/#225-contrast","text":"Resolution and contrast are closely related factors contributing to image quality. Contrast defines the differences in intensity values between the object under inspection and the background. Your imaging system should have enough contrast to distinguish objects from the background. Proper lighting techniques can enhance the contrast of your system.","title":"2.2.5 Contrast"},{"location":"COV%20816/02_B02/#226-depth-of-field","text":"The depth of field of a lens is its ability to keep objects of varying heights in focus. If you need to inspect objects with various heights, chose a lens that can maintain the image quality you need as the objects move closer to and further from the lens.","title":"2.2.6 Depth of Field"},{"location":"COV%20816/02_B02/#227-perspective","text":"Perspective errors often occur when the camera axis is not perpendicular to the object you are inspecting. Figure 3-3a shows an ideal camera position. Figure 3-3b shows a camera imaging an object from an angle. Figure 2.2-2. Camera Angle Relative to the Object of Interest.","title":"2.2.7 Perspective"},{"location":"COV%20816/02_B02/#23-calibration","text":"What is camera calibration? The process of estimating the parameters of a camera is called camera calibration. This means we have all the information (parameters or coefficients) about the camera required to determine an accurate relationship between a 3D point in the real world and its corresponding 2D projection (pixel) in the image captured by that calibrated camera. Typically this means recovering two kinds of parameters Internal parameters of the camera/lens system. E.g. focal length, optical center, and radial distortion coefficients of the lens. External parameters : This refers to the orientation (rotation and translation) of the camera with respect to some world coordinate system.","title":"2.3 Calibration"},{"location":"COV%20816/02_B02/#231-basics","text":"Some pinhole cameras introduce significant distortion to images. Two major kinds of distortion are radial distortion and tangential distortion. Radial distortion causes straight lines to appear curved. Radial distortion becomes larger the farther points are from the center of the image. For example, one image is shown below in which two edges of a chess board are marked with red lines. But, you can see that the border of the chess board is not a straight line and doesn't match with the red line. All the expected straight lines are bulged out. Figure 2.3-1. Radial Distortion. Radial distortion can be represented as follows: x_{distorted} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\\\ y_{distorted} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) Similarly, tangential distortion occurs because the image-taking lense is not aligned perfectly parallel to the imaging plane. So, some areas in the image may look nearer than expected. The amount of tangential distortion can be represented as below: x_{distorted} = x + [ 2p_1xy + p_2(r^2+2x^2)] \\\\ y_{distorted} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy] In short, we need to find five parameters, known as distortion coefficients given by: \\text{Distortion coefficients}=(k_1 \\hspace{10pt} k_2 \\hspace{10pt} p_1 \\hspace{10pt} p_2 \\hspace{10pt} k_3) In addition to this, we need to some other information, like the intrinsic and extrinsic parameters of the camera. Intrinsic parameters are specific to a camera. They include information like focal length (\\(f_x\\), \\(f_y\\)) and optical centers (\\(c_x\\), \\(c_y\\)). The focal length and optical centers can be used to create a camera matrix, which can be used to remove distortion due to the lenses of a specific camera. The camera matrix is unique to a specific camera, so once calculated, it can be reused on other images taken by the same camera. It is expressed as a \\(3\\times 3\\) matrix: \\text{Camera matrix} = \\left [ \\begin{matrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{matrix} \\right ] Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system.","title":"2.3.1 Basics"},{"location":"COV%20816/02_B02/#24-references","text":"[1] BRADSKI, G. The OpenCV Library , Dr. Dobb\u2019s Journal of Software Tools, 2000. [2] BURGER, W., BURGE, M. J. Principles of digital image processing. 1: Fundamental techniques . London, Springer, 2009. [3] NATIONAL INSTRUMENTS. IMAQ: Vision Concepts Manual . Austin, Texas, 2003.","title":"2.4 References"},{"location":"COV%20816/03_B03/","text":"3 IMAGE ANALYSIS AND PROCESSING [TEXT TO BE INCLUDED HERE]","title":"Unit 03 - Image Analysis and Processing"},{"location":"COV%20816/03_B03/#3-image-analysis-and-processing","text":"[TEXT TO BE INCLUDED HERE]","title":"3 IMAGE ANALYSIS AND PROCESSING"},{"location":"COV%20816/ementa/","text":"EMENTA DA DISCIPLINA COV 816 - PROCESSAMENTO DE IMAGENS (PROGRAMA DE ENG\u00aa OCE\u00c2NICA - 2023/1) Identifica\u00e7\u00e3o ITEM DESCRI\u00c7\u00c3O NOME: PROCESSAMENTO E AN\u00c1LISE DE IMAGENS NA ENGENHARIA NAVAL E OCE\u00c2NICA C\u00d3DIGO: COV-816 CARGA HOR\u00c1RIA: 45 HORAS \u00c1REA: ENGENHARIAS/ENGENHARIA NAVAL E OCE\u00c2NICA CR\u00c9DITOS: 3,0 P\u00fablico Alvo Estudantes de gradua\u00e7\u00e3o e p\u00f3s-gradua\u00e7\u00e3o que atuem nas \u00e1reas de engenharia. Objetivos GERAL: Proporcionar aos alunos um estudo aprofundado dos princ\u00edpios e t\u00e9cnicas de processamento de imagens nos seguintes t\u00f3picos: Aquisi\u00e7\u00e3o de imagens (equipamentos, amostragem, quantiza\u00e7\u00e3o e representa\u00e7\u00e3o de cores). Aprimoramento de imagens digitais no dom\u00ednios espacial (suaviza\u00e7\u00e3o de nitidez, detec\u00e7\u00e3o de bordas, limiariza\u00e7\u00e3o, equaliza\u00e7\u00e3o de histogramas, opera\u00e7\u00f5es morfol\u00f3gicas). Convers\u00e3o, segmenta\u00e7\u00e3o e representa\u00e7\u00e3o de imagens. Identifica\u00e7\u00e3o e seguimento de padr\u00f5es. Automa\u00e7\u00e3o dos modelos de processamento de imagens. ESPEC\u00cdFICO: Ao final do curso, os estudantes devem ser capazes de projetar e implementar operadores e processamentos diversos sobre imagens digitais de diversas modalidades e protocolos em problemas de estudo da Engenharia Naval e Oce\u00e2nica. Metodologia A disciplina est\u00e1 dividida em cinco blocos, cujos conte\u00fados ser\u00e3o ministrados com base em um plano particular de trabalho, e duas avalia\u00e7\u00f5es. Os blocos com seus conte\u00fados e planos de trabalho, e os pontos de avalia\u00e7\u00e3o, s\u00e3o discriminados na tabela a seguir: Bloco 1 - Conceitos b\u00e1sicos (4 aulas) CONTE\u00daDO: Defini\u00e7\u00e3o da Imagem Digital: Conceito geral da imagem digital e suas propriedades principais Tipos de Imagens e Arquivos de Imagem: Imagens em tons de cinza, imagens a cor, e imagens complexas. Espa\u00e7os de cor: Apresenta\u00e7\u00e3o dos principais espa\u00e7os de cor usados nas imagens digitais (RGB, HSL, CIE-Lab). Apresenta\u00e7\u00e3o do conte\u00fado do curso. PLANO DE TRABALHO: Aplica\u00e7\u00e3o ao vivo dos conceitos no software ImageJ/FIJI. Estudo da t\u00e9cnica de fotoluminesc\u00eancia. Opera\u00e7\u00f5es de cor e pseudo-cor nas imagens. Melhora digital das imagens. Bloco 2 - Aquisi\u00e7\u00e3o de imagens (4 aulas) CONTE\u00daDO: Aquisi\u00e7\u00e3o de imagens: Conceitos de Resolu\u00e7\u00e3o, Contraste, Profundidade de Campo, Perspectiva, Distor\u00e7\u00e3o. Calibra\u00e7\u00e3o Espacial: Processo de Calibra\u00e7\u00e3o, Sistema de Coordenadas, Algoritmos de Calibra\u00e7\u00e3o, Corre\u00e7\u00f5es de Imagem. PLANO DE TRABALHO: Aquisi\u00e7\u00e3o de imagens com webcam. Aquisi\u00e7\u00e3o de imagens com c\u00e2meras de uso industrial/cient\u00edfico. Rectifica\u00e7\u00e3o de imagens. Correla\u00e7\u00e3o de unidades de imagem e do mundo real. Protocolos de calibra\u00e7\u00e3o. EXAME PARCIAL (B1 - B2) Bloco 3 - An\u00e1lise e processamento de imagens (6 aulas) CONTE\u00daDO: An\u00e1lise de imagens: Histograma, Linha de Perfil, Medidas de Intensidade. Processamento de imagens: LUT, Convolu\u00e7\u00e3o, Filtros Espaciais, Binariza\u00e7\u00e3o, Morfologia de Imagens Bin\u00e1rias, Detec\u00e7\u00e3o de Bordas. PLANO DE TRABALHO: Filtragem das imagens retificadas no Bloco 2. Medi\u00e7\u00e3o de part\u00edculas e elementos da imagem. Programa\u00e7\u00e3o de Macros. Automa\u00e7\u00e3o do processamento e filtragem dos elementos da imagem digital. Bloco 4 - Metrologia por imagens (6 aulas) CONTE\u00daDO: Medi\u00e7\u00e3o em imagens: Medi\u00e7\u00e3o de regi\u00f5es, medi\u00e7\u00e3o de part\u00edculas. Segmenta\u00e7\u00e3o e Classifica\u00e7\u00e3o: T\u00e9cnicas de segmenta\u00e7\u00e3o de objetos e classifica\u00e7\u00e3o de part\u00edculas, automa\u00e7\u00e3o do processo de medi\u00e7\u00e3o/classifica\u00e7\u00e3o. PLANO DE TRABALHO: Filtragem das imagens retificadas no Bloco 2. Medi\u00e7\u00e3o de part\u00edculas e elementos da imagem. Programa\u00e7\u00e3o de Macros. Automa\u00e7\u00e3o do processamento e filtragem dos elementos da imagem digital. Bloco 5 - Apresenta\u00e7\u00f5es de Projetos por Equipe (2 aulas) CONTE\u00daDO: Discuss\u00e3o geral e resolu\u00e7\u00e3o de d\u00favidas gerais do curso. PLANO DE TRABALHO: Equipes apresentam projeto de processamento de imagens aplicado a problemas de relev\u00e2ncia da \u00e1rea de estudo. EXAME FINAL (B3 - B4) Ementa BLOCO DESCRI\u00c7\u00c3O B1 - Conceitos b\u00e1sicos Defini\u00e7\u00e3o da Imagem Digital: Conceito geral da imagem digital e suas propriedades principais. Tipos de Imagens e Arquivos de Imagem: Imagens em tons de cinza, imagens a cor, e imagens complexas. Espa\u00e7os de cor: Apresenta\u00e7\u00e3o dos espa\u00e7os de cor RGB, HSL e CIE-Lab usados nas imagens digitais. B2 - Aquisi\u00e7\u00e3o de imagens Aquisi\u00e7\u00e3o de imagens: Conceitos de Resolu\u00e7\u00e3o, Contraste, Profundidade de Campo, Perspectiva, Distor\u00e7\u00e3o. Calibra\u00e7\u00e3o Espacial: Processo de Calibra\u00e7\u00e3o, Sistema de Coordenadas, Algoritmos de Calibra\u00e7\u00e3o, Corre\u00e7\u00f5es de Imagem. B3 - An\u00e1lise e processamento de imagens An\u00e1lise de imagens: Histograma, Linha de Perfil, Medidas de Intensidade. Processamento de imagens: Convolu\u00e7\u00e3o, Filtros Espaciais, Binariza\u00e7\u00e3o, Morfologia de Imagens Bin\u00e1rias, Detec\u00e7\u00e3o de Bordas. B4 - Metrologia por imagens Medi\u00e7\u00e3o em imagens: Medi\u00e7\u00e3o de regi\u00f5es, medi\u00e7\u00e3o de part\u00edculas. Segmenta\u00e7\u00e3o e Classifica\u00e7\u00e3o: T\u00e9cnicas de segmenta\u00e7\u00e3o de objetos e classifica\u00e7\u00e3o de part\u00edculas, automa\u00e7\u00e3o do processo de medi\u00e7\u00e3o/classifica\u00e7\u00e3o. B5 - Apresenta\u00e7\u00e3o de Projetos Discuss\u00e3o geral e resolu\u00e7\u00e3o de d\u00favidas gerais do curso. Bibliografia Bibliografia Complementar ( Livro did\u00e1tico ) [1] BURGER, W., BURGE, M. J. Principles of digital image processing. 1: Fundamental techniques . London, Springer, 2009. Conceitos B\u00e1sicos [2] GONZALEZ, R. C., WOODS, R. E. Digital image processing . 2nd ed., Internat. Upper Saddle River, NJ, Prentice-Hall, 2002. [3] J\u00c4HNE, B. Digital image processing. 6th ed. Berlin\u202f; New York, Springer, 2005. [4] SZELISKI, R. Computer vision: algorithms and applications . London Heidelberg, Springer, 2011. Aquisi\u00e7\u00e3o, an\u00e1lise e processamento de imagens [5] CORKE, P. I. Robotics, vision and control: fundamental algorithms in MATLAB . Berlin, Springer, 2011. (Springer tracts in advanced robotics, v. 73). [6] RELF, C. G. Image acquisition and processing with LabVIEW. Boca Raton , CRC Press, 2004. (Image processing series). T\u00e9cnicas de Processamento de Imagens [7] BURGER, W., BURGE, M. J. Principles of digital image processing. 2: Core algorithms . London, Springer, 2009. [8] BURGER, W., BURGE, M. J. Principles of digital image processing. 3: Advanced methods . London, Springer, 2013. CHITYALA, R. Image processing and acquisition using Python . Boca Raton, FL, CRC Press, 2014. Trabalhos complementares de aplica\u00e7\u00e3o [9] ESCUDERO, M., HERN\u00c1NDEZ-FONTES, J. V., HERN\u00c1NDEZ, I. D., MENDOZA, E. Virtual Level Analysis Applied to Wave Flume Experiments: The Case of Waves-Cubipod Homogeneous Low-Crested Structure Interaction , Journal of Marine Science and Engineering, v. 9, n. 2, p. 230, 22 fev. 2021. Dispon\u00edvel em: http://dx.doi.org/10.3390/jmse9020230 [10] HERN\u00c1NDEZ, I. D., HERN\u00c1NDEZ-FONTES, J. V., VITOLA, M. A., SILVA, M.C., ESPERAN\u00c7A P.T.T. Water elevation measurements using binary image analysis for 2D hydrodynamic experiments , Ocean Engineering, v. 157, p. 325\u2013338, jun. 2018. Dispon\u00edvel em: http://dx.doi.org/10.1016/j.oceaneng.2018.03.063 [11] SCHNEIDER, C. A., RASBAND, W. S., & ELICEIRI, K. W. NIH Image to ImageJ: 25 years of image analysis . Nature Methods, 2012. 9(7), 671\u2013675. Dispon\u00edvel em: http://dx.doi.org/10.1038/nmeth.2089 [12] LIU, JUN-PENG; VAZ, MURILO A.; CHEN, RONG-QI; DUAN, MENG-LAN; HERN\u00c1NDEZ, IRVING D. Axial Mechanical Experiments of Unbonded Flexible Pipes . Petroleum Science 17, 5 (Outubro 2020): 1400\u20131410. Dispon\u00edvel em: http://dx.doi.org/10.1007/s12182-020-00504-3 Avalia\u00e7\u00e3o A avalia\u00e7\u00e3o do curso seguir\u00e1 os seguintes crit\u00e9rios: ITEM DESCRI\u00c7\u00c3O Exame parcial: 30% ( Conte\u00fado dos Blocos 1 e 2 ) Projeto (*): 30% ( Ser\u00e3o definidos no final do Bloco 2 ) Exame final: 40% ( Conte\u00fado dos Blocos 3 e 4 ) (*) O projeto poder\u00e1 ser desenvolvido individual ou coletivamente, e visa a aplica\u00e7\u00e3o dos conhecimentos adquiridos no curso para inferir propriedades, dimens\u00f5es ou caracter\u00edsticas de fen\u00f4menos/problemas da Engenharia Naval e Oce\u00e2nica a partir de imagens digitais 2D. Docente D.Sc. Irving David Hern\u00e1ndez Fontes","title":"Class Syllabus"},{"location":"COV%20816/ementa/#ementa-da-disciplina","text":"COV 816 - PROCESSAMENTO DE IMAGENS (PROGRAMA DE ENG\u00aa OCE\u00c2NICA - 2023/1)","title":"EMENTA DA DISCIPLINA"},{"location":"COV%20816/ementa/#identificacao","text":"ITEM DESCRI\u00c7\u00c3O NOME: PROCESSAMENTO E AN\u00c1LISE DE IMAGENS NA ENGENHARIA NAVAL E OCE\u00c2NICA C\u00d3DIGO: COV-816 CARGA HOR\u00c1RIA: 45 HORAS \u00c1REA: ENGENHARIAS/ENGENHARIA NAVAL E OCE\u00c2NICA CR\u00c9DITOS: 3,0","title":"Identifica\u00e7\u00e3o"},{"location":"COV%20816/ementa/#publico-alvo","text":"Estudantes de gradua\u00e7\u00e3o e p\u00f3s-gradua\u00e7\u00e3o que atuem nas \u00e1reas de engenharia.","title":"P\u00fablico Alvo"},{"location":"COV%20816/ementa/#objetivos","text":"GERAL: Proporcionar aos alunos um estudo aprofundado dos princ\u00edpios e t\u00e9cnicas de processamento de imagens nos seguintes t\u00f3picos: Aquisi\u00e7\u00e3o de imagens (equipamentos, amostragem, quantiza\u00e7\u00e3o e representa\u00e7\u00e3o de cores). Aprimoramento de imagens digitais no dom\u00ednios espacial (suaviza\u00e7\u00e3o de nitidez, detec\u00e7\u00e3o de bordas, limiariza\u00e7\u00e3o, equaliza\u00e7\u00e3o de histogramas, opera\u00e7\u00f5es morfol\u00f3gicas). Convers\u00e3o, segmenta\u00e7\u00e3o e representa\u00e7\u00e3o de imagens. Identifica\u00e7\u00e3o e seguimento de padr\u00f5es. Automa\u00e7\u00e3o dos modelos de processamento de imagens. ESPEC\u00cdFICO: Ao final do curso, os estudantes devem ser capazes de projetar e implementar operadores e processamentos diversos sobre imagens digitais de diversas modalidades e protocolos em problemas de estudo da Engenharia Naval e Oce\u00e2nica.","title":"Objetivos"},{"location":"COV%20816/ementa/#metodologia","text":"A disciplina est\u00e1 dividida em cinco blocos, cujos conte\u00fados ser\u00e3o ministrados com base em um plano particular de trabalho, e duas avalia\u00e7\u00f5es. Os blocos com seus conte\u00fados e planos de trabalho, e os pontos de avalia\u00e7\u00e3o, s\u00e3o discriminados na tabela a seguir:","title":"Metodologia"},{"location":"COV%20816/ementa/#bloco-1-conceitos-basicos-4-aulas","text":"CONTE\u00daDO: Defini\u00e7\u00e3o da Imagem Digital: Conceito geral da imagem digital e suas propriedades principais Tipos de Imagens e Arquivos de Imagem: Imagens em tons de cinza, imagens a cor, e imagens complexas. Espa\u00e7os de cor: Apresenta\u00e7\u00e3o dos principais espa\u00e7os de cor usados nas imagens digitais (RGB, HSL, CIE-Lab). Apresenta\u00e7\u00e3o do conte\u00fado do curso. PLANO DE TRABALHO: Aplica\u00e7\u00e3o ao vivo dos conceitos no software ImageJ/FIJI. Estudo da t\u00e9cnica de fotoluminesc\u00eancia. Opera\u00e7\u00f5es de cor e pseudo-cor nas imagens. Melhora digital das imagens.","title":"Bloco 1 - Conceitos b\u00e1sicos (4 aulas)"},{"location":"COV%20816/ementa/#bloco-2-aquisicao-de-imagens-4-aulas","text":"CONTE\u00daDO: Aquisi\u00e7\u00e3o de imagens: Conceitos de Resolu\u00e7\u00e3o, Contraste, Profundidade de Campo, Perspectiva, Distor\u00e7\u00e3o. Calibra\u00e7\u00e3o Espacial: Processo de Calibra\u00e7\u00e3o, Sistema de Coordenadas, Algoritmos de Calibra\u00e7\u00e3o, Corre\u00e7\u00f5es de Imagem. PLANO DE TRABALHO: Aquisi\u00e7\u00e3o de imagens com webcam. Aquisi\u00e7\u00e3o de imagens com c\u00e2meras de uso industrial/cient\u00edfico. Rectifica\u00e7\u00e3o de imagens. Correla\u00e7\u00e3o de unidades de imagem e do mundo real. Protocolos de calibra\u00e7\u00e3o. EXAME PARCIAL (B1 - B2)","title":"Bloco 2 - Aquisi\u00e7\u00e3o de imagens (4 aulas)"},{"location":"COV%20816/ementa/#bloco-3-analise-e-processamento-de-imagens-6-aulas","text":"CONTE\u00daDO: An\u00e1lise de imagens: Histograma, Linha de Perfil, Medidas de Intensidade. Processamento de imagens: LUT, Convolu\u00e7\u00e3o, Filtros Espaciais, Binariza\u00e7\u00e3o, Morfologia de Imagens Bin\u00e1rias, Detec\u00e7\u00e3o de Bordas. PLANO DE TRABALHO: Filtragem das imagens retificadas no Bloco 2. Medi\u00e7\u00e3o de part\u00edculas e elementos da imagem. Programa\u00e7\u00e3o de Macros. Automa\u00e7\u00e3o do processamento e filtragem dos elementos da imagem digital.","title":"Bloco 3 -  An\u00e1lise e processamento de imagens (6 aulas)"},{"location":"COV%20816/ementa/#bloco-4-metrologia-por-imagens-6-aulas","text":"CONTE\u00daDO: Medi\u00e7\u00e3o em imagens: Medi\u00e7\u00e3o de regi\u00f5es, medi\u00e7\u00e3o de part\u00edculas. Segmenta\u00e7\u00e3o e Classifica\u00e7\u00e3o: T\u00e9cnicas de segmenta\u00e7\u00e3o de objetos e classifica\u00e7\u00e3o de part\u00edculas, automa\u00e7\u00e3o do processo de medi\u00e7\u00e3o/classifica\u00e7\u00e3o. PLANO DE TRABALHO: Filtragem das imagens retificadas no Bloco 2. Medi\u00e7\u00e3o de part\u00edculas e elementos da imagem. Programa\u00e7\u00e3o de Macros. Automa\u00e7\u00e3o do processamento e filtragem dos elementos da imagem digital.","title":"Bloco 4 - Metrologia por imagens (6 aulas)"},{"location":"COV%20816/ementa/#bloco-5-apresentacoes-de-projetos-por-equipe-2-aulas","text":"CONTE\u00daDO: Discuss\u00e3o geral e resolu\u00e7\u00e3o de d\u00favidas gerais do curso. PLANO DE TRABALHO: Equipes apresentam projeto de processamento de imagens aplicado a problemas de relev\u00e2ncia da \u00e1rea de estudo. EXAME FINAL (B3 - B4)","title":"Bloco 5 - Apresenta\u00e7\u00f5es de Projetos por Equipe (2 aulas)"},{"location":"COV%20816/ementa/#ementa","text":"BLOCO DESCRI\u00c7\u00c3O B1 - Conceitos b\u00e1sicos Defini\u00e7\u00e3o da Imagem Digital: Conceito geral da imagem digital e suas propriedades principais. Tipos de Imagens e Arquivos de Imagem: Imagens em tons de cinza, imagens a cor, e imagens complexas. Espa\u00e7os de cor: Apresenta\u00e7\u00e3o dos espa\u00e7os de cor RGB, HSL e CIE-Lab usados nas imagens digitais. B2 - Aquisi\u00e7\u00e3o de imagens Aquisi\u00e7\u00e3o de imagens: Conceitos de Resolu\u00e7\u00e3o, Contraste, Profundidade de Campo, Perspectiva, Distor\u00e7\u00e3o. Calibra\u00e7\u00e3o Espacial: Processo de Calibra\u00e7\u00e3o, Sistema de Coordenadas, Algoritmos de Calibra\u00e7\u00e3o, Corre\u00e7\u00f5es de Imagem. B3 - An\u00e1lise e processamento de imagens An\u00e1lise de imagens: Histograma, Linha de Perfil, Medidas de Intensidade. Processamento de imagens: Convolu\u00e7\u00e3o, Filtros Espaciais, Binariza\u00e7\u00e3o, Morfologia de Imagens Bin\u00e1rias, Detec\u00e7\u00e3o de Bordas. B4 - Metrologia por imagens Medi\u00e7\u00e3o em imagens: Medi\u00e7\u00e3o de regi\u00f5es, medi\u00e7\u00e3o de part\u00edculas. Segmenta\u00e7\u00e3o e Classifica\u00e7\u00e3o: T\u00e9cnicas de segmenta\u00e7\u00e3o de objetos e classifica\u00e7\u00e3o de part\u00edculas, automa\u00e7\u00e3o do processo de medi\u00e7\u00e3o/classifica\u00e7\u00e3o. B5 - Apresenta\u00e7\u00e3o de Projetos Discuss\u00e3o geral e resolu\u00e7\u00e3o de d\u00favidas gerais do curso.","title":"Ementa"},{"location":"COV%20816/ementa/#bibliografia","text":"","title":"Bibliografia"},{"location":"COV%20816/ementa/#bibliografia-complementar-livro-didatico","text":"[1] BURGER, W., BURGE, M. J. Principles of digital image processing. 1: Fundamental techniques . London, Springer, 2009. Conceitos B\u00e1sicos [2] GONZALEZ, R. C., WOODS, R. E. Digital image processing . 2nd ed., Internat. Upper Saddle River, NJ, Prentice-Hall, 2002. [3] J\u00c4HNE, B. Digital image processing. 6th ed. Berlin\u202f; New York, Springer, 2005. [4] SZELISKI, R. Computer vision: algorithms and applications . London Heidelberg, Springer, 2011. Aquisi\u00e7\u00e3o, an\u00e1lise e processamento de imagens [5] CORKE, P. I. Robotics, vision and control: fundamental algorithms in MATLAB . Berlin, Springer, 2011. (Springer tracts in advanced robotics, v. 73). [6] RELF, C. G. Image acquisition and processing with LabVIEW. Boca Raton , CRC Press, 2004. (Image processing series). T\u00e9cnicas de Processamento de Imagens [7] BURGER, W., BURGE, M. J. Principles of digital image processing. 2: Core algorithms . London, Springer, 2009. [8] BURGER, W., BURGE, M. J. Principles of digital image processing. 3: Advanced methods . London, Springer, 2013. CHITYALA, R. Image processing and acquisition using Python . Boca Raton, FL, CRC Press, 2014.","title":"Bibliografia Complementar (Livro did\u00e1tico)"},{"location":"COV%20816/ementa/#trabalhos-complementares-de-aplicacao","text":"[9] ESCUDERO, M., HERN\u00c1NDEZ-FONTES, J. V., HERN\u00c1NDEZ, I. D., MENDOZA, E. Virtual Level Analysis Applied to Wave Flume Experiments: The Case of Waves-Cubipod Homogeneous Low-Crested Structure Interaction , Journal of Marine Science and Engineering, v. 9, n. 2, p. 230, 22 fev. 2021. Dispon\u00edvel em: http://dx.doi.org/10.3390/jmse9020230 [10] HERN\u00c1NDEZ, I. D., HERN\u00c1NDEZ-FONTES, J. V., VITOLA, M. A., SILVA, M.C., ESPERAN\u00c7A P.T.T. Water elevation measurements using binary image analysis for 2D hydrodynamic experiments , Ocean Engineering, v. 157, p. 325\u2013338, jun. 2018. Dispon\u00edvel em: http://dx.doi.org/10.1016/j.oceaneng.2018.03.063 [11] SCHNEIDER, C. A., RASBAND, W. S., & ELICEIRI, K. W. NIH Image to ImageJ: 25 years of image analysis . Nature Methods, 2012. 9(7), 671\u2013675. Dispon\u00edvel em: http://dx.doi.org/10.1038/nmeth.2089 [12] LIU, JUN-PENG; VAZ, MURILO A.; CHEN, RONG-QI; DUAN, MENG-LAN; HERN\u00c1NDEZ, IRVING D. Axial Mechanical Experiments of Unbonded Flexible Pipes . Petroleum Science 17, 5 (Outubro 2020): 1400\u20131410. Dispon\u00edvel em: http://dx.doi.org/10.1007/s12182-020-00504-3","title":"Trabalhos complementares de aplica\u00e7\u00e3o"},{"location":"COV%20816/ementa/#avaliacao","text":"A avalia\u00e7\u00e3o do curso seguir\u00e1 os seguintes crit\u00e9rios: ITEM DESCRI\u00c7\u00c3O Exame parcial: 30% ( Conte\u00fado dos Blocos 1 e 2 ) Projeto (*): 30% ( Ser\u00e3o definidos no final do Bloco 2 ) Exame final: 40% ( Conte\u00fado dos Blocos 3 e 4 ) (*) O projeto poder\u00e1 ser desenvolvido individual ou coletivamente, e visa a aplica\u00e7\u00e3o dos conhecimentos adquiridos no curso para inferir propriedades, dimens\u00f5es ou caracter\u00edsticas de fen\u00f4menos/problemas da Engenharia Naval e Oce\u00e2nica a partir de imagens digitais 2D.","title":"Avalia\u00e7\u00e3o"},{"location":"COV%20816/ementa/#docente","text":"D.Sc. Irving David Hern\u00e1ndez Fontes","title":"Docente"}]}